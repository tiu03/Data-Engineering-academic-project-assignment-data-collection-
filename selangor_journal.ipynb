{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c33f6e-f9a6-408e-b9b4-b3e5c70f12af",
   "metadata": {},
   "source": [
    "# 1.0 Data Collection (Web Scraping)\n",
    "\n",
    "###### Author: Terence Tiu Chuan Jie \n",
    "###### Last Edited: 10/4/2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebb90f-ed15-4c65-8ca1-57981ffc68cc",
   "metadata": {},
   "source": [
    "## selangor_journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50909ff3-56d4-4f78-834a-818ed5882d43",
   "metadata": {},
   "source": [
    "###### tips for downloadchrome drive for selenium and the driver https://github.com/password123456/setup-selenium-with-chrome-driver-on-ubuntu_debian#step-2-download-google-chrome-stable-package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad14ee9f-e336-4904-b108-db3157eabbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/14 21:55:18 WARN Utils: Your hostname, tiu. resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/14 21:55:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/14 21:55:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(r'/home/student/data_collected')\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "spark = SparkSession.builder.appName('selangorjournal').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd65a7d-a071-4dbd-8b2e-53bdf1a298cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from fake_useragent import UserAgent\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "class SelangorJournalScraper:\n",
    "    def __init__(self, base_url, pages=2):\n",
    "        self.base_url = base_url\n",
    "        self.pages = pages\n",
    "        self.section_urls = self.generate_section_urls()\n",
    "        self.news_urls = []\n",
    "        self.data = []\n",
    "\n",
    "    def generate_section_urls(self):\n",
    "        urls = [self.base_url]\n",
    "        for page in range(2, self.pages + 1):\n",
    "            urls.append(f\"{self.base_url}page/{page}/\")\n",
    "        return urls\n",
    "\n",
    "    def _create_driver(self):\n",
    "        user_agent = UserAgent()\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(f\"user-agent={user_agent.random}\")\n",
    "        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def collect_article_urls(self):\n",
    "        for section_url in self.section_urls:\n",
    "            driver = self._create_driver()\n",
    "            try:\n",
    "                driver.get(section_url)\n",
    "                time.sleep(2)\n",
    "                url_elements = driver.find_elements(By.CSS_SELECTOR, '.penci-link-post.penci-image-holder.penci-disable-lazy')\n",
    "                for element in url_elements:\n",
    "                    href = element.get_attribute('href')\n",
    "                    if href:\n",
    "                        self.news_urls.append(href)\n",
    "            except Exception as e:\n",
    "                print(f\"Error on section page {section_url}: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "        print(f\"Collected {len(self.news_urls)} article URLs.\")\n",
    "\n",
    "    def scrape_articles(self):\n",
    "        for url in self.news_urls:\n",
    "            driver = self._create_driver()\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"h1\"))\n",
    "                )\n",
    "\n",
    "                headline = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "                date = driver.find_element(By.TAG_NAME, \"time\").get_attribute('datetime')\n",
    "\n",
    "                content = ''\n",
    "                possible_classes = [\"dable-content-wrapper\", \"entry-content\", \"article-content\"]\n",
    "                for cls in possible_classes:\n",
    "                    if driver.find_elements(By.CLASS_NAME, cls):\n",
    "                        wrapper = driver.find_element(By.CLASS_NAME, cls)\n",
    "                        paragraphs = wrapper.find_elements(By.TAG_NAME, \"p\")\n",
    "                        content = ' '.join([p.text for p in paragraphs])\n",
    "                        break\n",
    "\n",
    "                if content:\n",
    "                    self.data.append({\n",
    "                        'url': url,\n",
    "                        'headline': headline,\n",
    "                        'date of published': date,\n",
    "                        'article_content': content\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"No content found on {url}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {url}: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "        print(f\"Scraped {len(self.data)} articles.\")\n",
    "\n",
    "    def export_to_csv(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "                fieldnames = ['url', 'headline', 'date of published', 'article_content']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for item in self.data:\n",
    "                    writer.writerow(item)\n",
    "            print(f\"Exported to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write CSV: {e}\")\n",
    "\n",
    "\n",
    "# # === Example Usage ===\n",
    "# if __name__ == \"__main__\":\n",
    "#     scraper = SelangorJournalScraper(\n",
    "#         base_url='https://selangorjournal.my/category/current/crime/',\n",
    "#         pages=2\n",
    "#     )\n",
    "\n",
    "#     scraper.collect_article_urls()\n",
    "#     scraper.scrape_articles()\n",
    "#     scraper.export_to_csv('selangor_journal_new_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44be976-bd4d-4f5a-a8b1-548b8d6ac19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f28d452-aa0e-4d35-95d6-f2cd5ede0be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501c572-0969-48e9-9fb0-50b84267f33c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a884d9-d323-421c-9e21-0450eb8711db",
   "metadata": {},
   "source": [
    "## with kafka "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a78b8f-bd3d-4e0a-9b81-f1157e01e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "class SelangorJournalProducer:\n",
    "    def __init__(self, kafka_server='localhost:9092', topic='selangor_journal_topic'):\n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=kafka_server,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        self.topic = topic\n",
    "\n",
    "    def send_article(self, article):\n",
    "        self.producer.send(self.topic, article)\n",
    "        print(f\"Sent: {article['headline']}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.producer.flush()\n",
    "        self.producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4080d411-ffae-4af7-a5a4-cb790a996afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from fake_useragent import UserAgent\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "class SelangorJournalScraper:\n",
    "    def __init__(self, base_url, pages=2):\n",
    "        self.base_url = base_url\n",
    "        self.pages = pages\n",
    "        self.section_urls = self.generate_section_urls()\n",
    "        self.news_urls = []\n",
    "        self.data = []\n",
    "\n",
    "    def generate_section_urls(self):\n",
    "        urls = [self.base_url]\n",
    "        for page in range(2, self.pages + 1):\n",
    "            urls.append(f\"{self.base_url}page/{page}/\")\n",
    "        return urls\n",
    "\n",
    "    def _create_driver(self):\n",
    "        user_agent = UserAgent()\n",
    "        options = Options()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(f\"user-agent={user_agent.random}\")\n",
    "        return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "    def collect_article_urls(self):\n",
    "        for section_url in self.section_urls:\n",
    "            driver = self._create_driver()\n",
    "            try:\n",
    "                driver.get(section_url)\n",
    "                time.sleep(2)\n",
    "                url_elements = driver.find_elements(By.CSS_SELECTOR, '.penci-link-post.penci-image-holder.penci-disable-lazy')\n",
    "                for element in url_elements:\n",
    "                    href = element.get_attribute('href')\n",
    "                    if href:\n",
    "                        self.news_urls.append(href)\n",
    "            except Exception as e:\n",
    "                print(f\"Error on section page {section_url}: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "        print(f\"Collected {len(self.news_urls)} article URLs.\")\n",
    "\n",
    "    def scrape_and_stream_articles(self, producer):\n",
    "        for url in self.news_urls:\n",
    "            driver = self._create_driver()\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"h1\"))\n",
    "                )\n",
    "\n",
    "                headline = driver.find_element(By.TAG_NAME, \"h1\").text\n",
    "                date = driver.find_element(By.TAG_NAME, \"time\").get_attribute('datetime')\n",
    "\n",
    "                content = ''\n",
    "                possible_classes = [\"dable-content-wrapper\", \"entry-content\", \"article-content\"]\n",
    "                for cls in possible_classes:\n",
    "                    if driver.find_elements(By.CLASS_NAME, cls):\n",
    "                        wrapper = driver.find_element(By.CLASS_NAME, cls)\n",
    "                        paragraphs = wrapper.find_elements(By.TAG_NAME, \"p\")\n",
    "                        content = ' '.join([p.text for p in paragraphs])\n",
    "                        break\n",
    "\n",
    "                if content:\n",
    "                    article_data = {\n",
    "                        'url': url,\n",
    "                        'headline': headline,\n",
    "                        'date_published': date,\n",
    "                        'article_content': content\n",
    "                    }\n",
    "                    producer.send_article(article_data)\n",
    "                else:\n",
    "                    print(f\"No content found on {url}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping {url}: {e}\")\n",
    "            finally:\n",
    "                driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4729b807-d470-4034-b6ca-95f7e4a2eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 24 article URLs.\n",
      "Sent: Syndicate distributing drugs to Sarawak dismantled, syabu worth over RM3 mln seized\n",
      "Sent: Investment fraud syndicated crippled with over RM3 bln in assets seized\n",
      "Sent: High Court upholds consultant’s acquittal over RM1.8 mln in cheating, money laundering\n",
      "Sent: Couple held over alleged abuse at Cheras shelter home\n",
      "Sent: RM177mln worth of cash, gold, seized by MACC in Ismail Sabri case\n",
      "Sent: Man held for allegedly threatening DBKL enforcement officers on social media\n",
      "Sent: Police nab three kidnapping suspects after opening fire at car\n",
      "Sent: Op Cyber Guardian: Four arrested, over 50,000 child sexual abuse material seized\n",
      "Sent: Police thwart attempt to smuggle drug worth RM1.14 mln at KLIA\n",
      "Sent: Armed Forces personnel who attacked woman driver released on bail\n",
      "Sent: Company auditor loses nearly RM1.3 mln to investment scam\n",
      "Sent: Woman, son charged with child abuse of two boys\n",
      "Sent: Freelance preacher remanded for two days over alleged sedition\n",
      "Sent: Cops arrest freelance preacher over alleged seditious post\n",
      "Sent: Woman charged in Mitra funds case pleads not guilty\n",
      "Sent: RM19mln lost due to online fraud this year — Bukit Aman\n",
      "Sent: Deputy IGP urges update of Poisons Act chemical list to aid prosecution\n",
      "Sent: Cops find meth, fentanyl traces in Lembah Subang sewers, on hunt for drug labs\n",
      "Sent: Court bins bail bid by GISBH leader, 12 others\n",
      "Sent: Police investigate man over alleged lewd act in university library\n",
      "Sent: Woman arrested after splashing hot water on daughter\n",
      "Sent: Agent nabbed for alleged bribery in vehicle inspection queue manipulation\n",
      "Sent: Labourer gets 33 years, 20 strokes of the cane for rape, trespass\n",
      "Sent: Driver in hit-and-run charged with attempted murder\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    scraper = SelangorJournalScraper(\n",
    "        base_url='https://selangorjournal.my/category/current/crime/',\n",
    "        pages=2\n",
    "    )\n",
    "\n",
    "    producer = SelangorJournalProducer()\n",
    "\n",
    "    scraper.collect_article_urls()\n",
    "    scraper.scrape_and_stream_articles(producer)\n",
    "\n",
    "    producer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e0ed37-eb4d-4b3b-a979-cb1946288791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consuming messages...\n",
      "Saved: Syndicate distributing drugs to Sarawak dismantled, syabu wo\n",
      "Saved: Investment fraud syndicated crippled with over RM3 bln in as\n",
      "Saved: High Court upholds consultant’s acquittal over RM1.8 mln in \n",
      "Saved: Couple held over alleged abuse at Cheras shelter home\n",
      "Saved: RM177mln worth of cash, gold, seized by MACC in Ismail Sabri\n",
      "Saved: Man held for allegedly threatening DBKL enforcement officers\n",
      "Saved: Police nab three kidnapping suspects after opening fire at c\n",
      "Saved: Op Cyber Guardian: Four arrested, over 50,000 child sexual a\n",
      "Saved: Police thwart attempt to smuggle drug worth RM1.14 mln at KL\n",
      "Saved: Armed Forces personnel who attacked woman driver released on\n",
      "Saved: Company auditor loses nearly RM1.3 mln to investment scam\n",
      "Saved: Woman, son charged with child abuse of two boys\n",
      "Saved: Freelance preacher remanded for two days over alleged sediti\n",
      "Saved: Cops arrest freelance preacher over alleged seditious post\n",
      "Saved: Woman charged in Mitra funds case pleads not guilty\n",
      "Saved: RM19mln lost due to online fraud this year — Bukit Aman\n",
      "Saved: Deputy IGP urges update of Poisons Act chemical list to aid \n",
      "Saved: Cops find meth, fentanyl traces in Lembah Subang sewers, on \n",
      "Saved: Court bins bail bid by GISBH leader, 12 others\n",
      "Saved: Police investigate man over alleged lewd act in university l\n",
      "Saved: Woman arrested after splashing hot water on daughter\n",
      "Saved: Agent nabbed for alleged bribery in vehicle inspection queue\n",
      "Saved: Labourer gets 33 years, 20 strokes of the cane for rape, tre\n",
      "Saved: Driver in hit-and-run charged with attempted murder\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "class SelangorJournalConsumer:\n",
    "    def __init__(self, kafka_server='localhost:9092', topic='selangor_journal_topic'):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=kafka_server,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            value_deserializer=lambda v: json.loads(v.decode('utf-8'))\n",
    "        )\n",
    "\n",
    "    def consume_and_save(self, output_file='selangor_journal_output.csv'):\n",
    "        print(\"Consuming messages...\")\n",
    "        fieldnames = ['url', 'headline', 'date_published', 'article_content']\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8-sig', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for message in self.consumer:\n",
    "                article = message.value\n",
    "                writer.writerow(article)\n",
    "                print(f\"Saved: {article['headline'][:60]}\")\n",
    "\n",
    "# Example:\n",
    "consumer = SelangorJournalConsumer()\n",
    "consumer.consume_and_save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f667b-1ea2-48cb-a1b2-efbb6d38b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09093f7c-18c1-40cd-a9b5-68687f50c2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-venv",
   "language": "python",
   "name": "de-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
